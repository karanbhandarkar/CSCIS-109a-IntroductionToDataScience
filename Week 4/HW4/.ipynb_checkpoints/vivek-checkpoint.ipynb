{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"fig/iacs.png\"> S-109A Introduction to Data Science \n",
    "## Homework 4 - Regularization \n",
    "\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Summer 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "Names of people you have worked with goes here: \n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling(): styles = open(\"cs109.css\", \"r\").read(); return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import these libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "\n",
    "from pandas.core import datetools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Bike Sharing Usage Data\n",
    "\n",
    "In this homework, we will focus on regularization and cross validation. We will continue to build regression models for the Capital Bikeshare program in Washington D.C.  See homework 3 for more information about the Capital Bikeshare data that we'll be using extensively. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "\n",
    "\n",
    "<div class='exercise'> <b> Question 1 </b> </div> \n",
    "  In HW3 Questions 1-3, you preprocessed the data in preparation for your regression analysis. We ask you to repeat those steps (particularly those in Question 3) so that we can compare the analysis models in this HW with those you developed in HW3.  In this HW we'll be using models from sklearn exclusively (as opposed to statsmodels)\n",
    "  \n",
    "**1.1** [From HW3] Read `data/BSS_train.csv` and `data/BSS_test.csv` into dataframes `BSS_train` and `BSS_test`, respectively.  Remove the `dteday` column from both train and test dataset. We do not need it, and its format cannot be used for analysis.  Also remove the `casual` and `registered` columns for both training and test datasets as they make  `count` trivial.   \n",
    "\n",
    "**1.2** Since we'll be exploring Regularization and Polynomial Features, it will make sense to standardize our data.  Standardize the numerical features. Store the dataframes for the processed training and test predictors into the variables `X_train` and `X_test`.  Store the appropriately shaped numpy arrays for the corresponding train and test `count` columns into `y_train` and `y_test`.\n",
    "\n",
    "**1.3** Use the `LinearRegression` library from `sklearn` to fit a multiple linear regression model to the training set data in `X_train`.  Store the fitted model in the variable `BikeOLSModel`.\n",
    "\n",
    "**1.4** What are the training and test set $R^2$ scores?  Store the training and test $R^2$ scores of the `BikeOLSModel` in a dictionary `BikeOLS_r2scores` using the string 'training' and 'test' as keys.  \n",
    "\n",
    "**1.5**   We're going to use bootstrapped confidence intervals (use 500 bootstrap iterations) to determine which of the estimated coefficients for the `BikeOLSModel` are statistically significant at a significance level of 5% .  We'll do so by creating 3 different functions:\n",
    "\n",
    "1. `make_bootstrap_sample(dataset_X, dataset_y)` returns a bootstrap sample of `dataset_X` and `dataset_y`\n",
    "2. `calculate_coefficients(dataset_X, dataset_y, model)` returns in the form of a dictionary regression coefficients calculated by your model on `dataset_X` and `dataset_y`.  The keys for regression coefficients dictionary should be the names of the features.  The values should be the coefficient values of that feature calculated on your model.  An example would be {'hum': 12.3, 'windspeed': -1.2, 'Sunday': 0.6 ... }\n",
    "3. `get_significant_predictors(regression_coefficients, significance_level)` takes as input a list of regression coefficient dictionaries (each one the output of `calculate_coefficients` and  returns a python list of the feature names of the significant predictors e.g. ['Monday', 'hum', 'holiday', ... ]\n",
    "\n",
    "In the above functions `dataset_X` should always be a pandas dataframe with your features, `dataset_y` a numpy column vector with the values of the response variable and collectively they form the dataset upon which the operations take place. `model` is the `sklearn` regression model that will be used to generate the regression coefficients. `regression_coefficients` is a list of dictionaries of numpy arrays with each numpy array containing the regression coefficients (not including the intercept) calculated from one bootstrap sample.  `significance_level` represents the significance level as a floating point number.  So a 5% significance level should be represented as 0.05.  \n",
    "\n",
    "\n",
    "Store the feature names as a list of strings in the variable `BikeOLS_significant_bootstrap` and print them for your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Read `data/BSS_train.csv` and `data/BSS_test.csv` into Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "BSS_train = pd.read_csv(\"data/BSS_train.csv\", index_col=0)\n",
    "BSS_test = pd.read_csv(\"data/BSS_test.csv\", index_col=0)\n",
    "\n",
    "BSS_train = BSS_train.drop(['dteday','year','casual','registered'], axis=1)\n",
    "BSS_test = BSS_test.drop(['dteday','year','casual','registered'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Standardizing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizeCounts(data):\n",
    "    mean = np.mean(data[\"counts\"])\n",
    "    std = np.std(data[\"counts\"])\n",
    "    data[\"counts\"]= np.abs((data[\"counts\"]-mean))/std\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49673164065310915\n",
      "0.19197884163226706\n"
     ]
    }
   ],
   "source": [
    "df_with_cols2standardize = BSS_train[[\"temp\", \"atemp\", \"hum\", \"windspeed\"]]\n",
    "df_with_cols2standardize.head()\n",
    "df_with_cols2standardize= np.abs((df_with_cols2standardize-df_with_cols2standardize.mean()))/df_with_cols2standardize.std()\n",
    "BSS_train.head()\n",
    "\n",
    "print(np.mean(BSS_train['temp']))\n",
    "print(np.std(BSS_test['temp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.332291</td>\n",
       "      <td>1.090575</td>\n",
       "      <td>0.937075</td>\n",
       "      <td>1.557459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.436080</td>\n",
       "      <td>1.178973</td>\n",
       "      <td>0.885288</td>\n",
       "      <td>1.557459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.436080</td>\n",
       "      <td>1.178973</td>\n",
       "      <td>0.885288</td>\n",
       "      <td>1.557459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.332291</td>\n",
       "      <td>1.090575</td>\n",
       "      <td>0.626354</td>\n",
       "      <td>1.557459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.332291</td>\n",
       "      <td>1.090575</td>\n",
       "      <td>0.626354</td>\n",
       "      <td>1.557459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       temp     atemp       hum  windspeed\n",
       "0  1.332291  1.090575  0.937075   1.557459\n",
       "1  1.436080  1.178973  0.885288   1.557459\n",
       "2  1.436080  1.178973  0.885288   1.557459\n",
       "3  1.332291  1.090575  0.626354   1.557459\n",
       "4  1.332291  1.090575  0.626354   1.557459"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_cols2standardize.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>counts</th>\n",
       "      <th>spring</th>\n",
       "      <th>summer</th>\n",
       "      <th>...</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Mon</th>\n",
       "      <th>Tue</th>\n",
       "      <th>Wed</th>\n",
       "      <th>Thu</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Sat</th>\n",
       "      <th>Cloudy</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Storm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.923062</td>\n",
       "      <td>0.459180</td>\n",
       "      <td>0.170252</td>\n",
       "      <td>1.249501</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.124890</td>\n",
       "      <td>0.624573</td>\n",
       "      <td>0.072994</td>\n",
       "      <td>1.249501</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.124890</td>\n",
       "      <td>0.624573</td>\n",
       "      <td>0.072994</td>\n",
       "      <td>1.249501</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.923062</td>\n",
       "      <td>0.459180</td>\n",
       "      <td>0.413298</td>\n",
       "      <td>1.249501</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.923062</td>\n",
       "      <td>0.459180</td>\n",
       "      <td>0.413298</td>\n",
       "      <td>1.249501</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour  holiday  workingday      temp     atemp       hum  windspeed  counts  \\\n",
       "0     0        0           0  0.923062  0.459180  0.170252   1.249501      16   \n",
       "1     1        0           0  1.124890  0.624573  0.072994   1.249501      40   \n",
       "2     2        0           0  1.124890  0.624573  0.072994   1.249501      32   \n",
       "3     3        0           0  0.923062  0.459180  0.413298   1.249501      13   \n",
       "4     4        0           0  0.923062  0.459180  0.413298   1.249501       1   \n",
       "\n",
       "   spring  summer  ...    Dec  Mon  Tue  Wed  Thu  Fri  Sat  Cloudy  Snow  \\\n",
       "0       0       0  ...      0    0    0    0    0    0    1       0     0   \n",
       "1       0       0  ...      0    0    0    0    0    0    1       0     0   \n",
       "2       0       0  ...      0    0    0    0    0    0    1       0     0   \n",
       "3       0       0  ...      0    0    0    0    0    0    1       0     0   \n",
       "4       0       0  ...      0    0    0    0    0    0    1       0     0   \n",
       "\n",
       "   Storm  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "BSS_train[\"temp\"]= cols2standardize[\"temp\"]\n",
    "BSS_train[\"atemp\"]= cols2standardize[\"atemp\"]\n",
    "BSS_train[\"hum\"]= cols2standardize[\"hum\"]\n",
    "BSS_train[\"windspeed\"]= cols2standardize[\"windspeed\"]\n",
    "BSS_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BSS_train = standardizeCounts(BSS_train)\n",
    "#BSS_test = standardizeCounts(BSS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = BSS_train[\"counts\"]\n",
    "y_test = BSS_test[\"counts\"]\n",
    "x_train = BSS_train.drop(\"counts\", axis=1)\n",
    "x_test = BSS_test.drop(\"counts\", axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Use the `LinearRegression` library from `sklearn` to fit a multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "BikeOLSModel = LinearRegression().fit(x_train, y_train)\n",
    "coeff = BikeOLSModel.coef_\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 What are the training and test set $R^2$ scores? Store the $R^2$ scores of the `BikeOLSModel` on the training and test sets in a dictionary `BikeOLS_r2scores`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': 0.2779734521371273, 'test': 0.248366840428361}\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "r2score_train = BikeOLSModel.score(x_train,y_train)\n",
    "r2score_test = BikeOLSModel.score(x_test,y_test)\n",
    "\n",
    "BikeOLS_r2scores = dict()\n",
    "\n",
    "BikeOLS_r2scores[\"training\"] = r2score_train\n",
    "BikeOLS_r2scores[\"test\"] = r2score_test\n",
    "\n",
    "print(BikeOLS_r2scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 We're going to use bootstrapped confidence intervals to determine which of the estimated coefficients ...\n",
    "\n",
    "**1.5**   We're going to use bootstrapped confidence intervals (use 500 bootstrap iterations) to determine which of the estimated coefficients for the `BikeOLSModel` are statistically significant at a significance level of 5% .  We'll do so by creating 3 different functions:\n",
    "\n",
    "1. `make_bootstrap_sample(dataset_X, dataset_y)` returns a bootstrap sample of `dataset_X` and `dataset_y`\n",
    "2. `calculate_coefficients(dataset_X, dataset_y, model)` returns in the form of a dictionary regression coefficients calculated by your model on `dataset_X` and `dataset_y`.  The keys for regression coefficients dictionary should be the names of the features.  The values should be the coefficient values of that feature calculated on your model.  An example would be {'hum': 12.3, 'windspeed': -1.2, 'Sunday': 0.6 ... }\n",
    "3. `get_significant_predictors(regression_coefficients, significance_level)` takes as input a list of regression coefficient dictionaries (each one the output of `calculate_coefficients` and  returns a python list of the feature names of the significant predictors e.g. ['Monday', 'hum', 'holiday', ... ]\n",
    "\n",
    "In the above functions `dataset_X` should always be a pandas dataframe with your features, `dataset_y` a numpy column vector with the values of the response variable and collectively they form the dataset upon which the operations take place. `model` is the `sklearn` regression model that will be used to generate the regression coefficients. `regression_coefficients` is a list of dictionaries of numpy arrays with each numpy array containing the regression coefficients (not including the intercept) calculated from one bootstrap sample.  `significance_level` represents the significance level as a floating point number.  So a 5% significance level should be represented as 0.05.  \n",
    "\n",
    "\n",
    "Store the feature names as a list of strings in the variable `BikeOLS_significant_bootstrap` and print them for your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# dataset_x should be a pandas dataframe\n",
    "\n",
    "## accepts dataset inputs as numpy arrays\n",
    "\n",
    "def make_bootstrap_sample(dataset_X, dataset_y, size = None):\n",
    "    \n",
    "    # by default return a bootstrap sample of the same size as the original dataset\n",
    "    if not size: size = len(dataset_X)\n",
    "        \n",
    "    # if the X and y datasets aren't the same size, raise an exception\n",
    "    if len(dataset_X) != len(dataset_y):\n",
    "        raise Exception(\"Data size must match between dataset_X and dataset_y\")\n",
    "    \n",
    "    indices_to_sample = np.random.choice(dataset_X.shape[0], dataset_X.shape[0], replace=True)\n",
    "    bootstrap_dataset_X = dataset_X.values[indices_to_sample]\n",
    "    bootstrap_dataset_y = dataset_y.values[indices_to_sample]\n",
    "    \n",
    "    # return as a tuple your bootstrap samples of dataset_X as a pandas dataframe\n",
    "    # and your bootstrap samples of dataset y as a numpy column vector\n",
    "    \n",
    "    return (bootstrap_dataset_X, bootstrap_dataset_y)\n",
    "    \n",
    "\n",
    "def calculate_coefficients(dataset_X, dataset_y, model):\n",
    "    \n",
    "    # your code here\n",
    "    coefficients_dictionary = dict()\n",
    "    coefficiants = model.coef_\n",
    "    index = 0\n",
    "    for column in dataset_X.columns:\n",
    "        coefficients_dictionary[column] = coefficiants[index]\n",
    "        index+=1\n",
    "    # return coefficients  in the variable  coefficients_dictioanry as a dictionary\n",
    "    # with the key being the name of the feature as a string\n",
    "    # the value being the value of the coefficients\n",
    "    # do not return the intercept as part of this\n",
    "    return coefficients_dictionary\n",
    "\n",
    "\n",
    "def get_significant_predictors(regression_coefficients, significance_level, dataset_X):\n",
    "    \n",
    "    # your code here\n",
    "    significant_coefficients =[]\n",
    "    feature_coeff_map={}\n",
    "    for column in dataset_X.columns:\n",
    "        feature_coeff = []\n",
    "        for dictionary in regression_coefficients:\n",
    "            for c, coeff in dictionary.items():\n",
    "                if(c==column):\n",
    "                    feature_coeff.append(coeff)\n",
    "                    \n",
    "        \n",
    "        feature_coeff_map[column] = feature_coeff\n",
    "        #print(len(feature_coeff))\n",
    "    \n",
    "    for feature, coeff in feature_coeff_map.items():\n",
    "        #print(\"Evaluating Significance of feature \", feature )\n",
    "        lower_limit, upper_limit = np.percentile(coeff, [5,95])\n",
    "        #print(\"lower limit \", lower_limit)\n",
    "        if(lower_limit<significance_level):\n",
    "            significant_coefficients.append(feature)\n",
    "    \n",
    "    #print((feature_coeff_map[\"windspeed\"]))\n",
    "    #print((feature_coeff_map[\"hour\"]))\n",
    "    # regression_coefficients is a list of dictionaries\n",
    "    # with the key being the name of the feature as a string\n",
    "    # the value being the value of the coefficients\n",
    "    # each dictionary in th list should be the output of calculate_coefficients\n",
    "    \n",
    "    # return the significant coefficients as a list of strings\n",
    "    return significant_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant Preditors are ,  ['holiday', 'workingday', 'atemp', 'Nov', 'Dec', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Cloudy', 'Snow', 'Storm']\n"
     ]
    }
   ],
   "source": [
    "N = 500\n",
    "significance_level = .05\n",
    "regression_coefficients = []\n",
    "for i in range(N):\n",
    "    bootstrap_dataset_X, bootstrap_dataset_y = make_bootstrap_sample(x_train, y_train)\n",
    "    bootstrap_model = LinearRegression().fit(bootstrap_dataset_X, bootstrap_dataset_y)\n",
    "    coefficients_dictionary = calculate_coefficients(x_train, y_test, bootstrap_model)\n",
    "    regression_coefficients.append(coefficients_dictionary)\n",
    "    \n",
    "\n",
    "\n",
    "sig_predictors = get_significant_predictors(regression_coefficients, significance_level, x_train)\n",
    "print(\"Significant Preditors are , \",sig_predictors)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalization Methods\n",
    "\n",
    "In HW 3 Question 5 we explored using subset selection to find a significant subset of features.  We then fit a regression model just on that subset of features instead of on the full dataset (including all features).   As an alternative to selecting a subset of predictors and fitting a regression model on the subset, one can fit a linear regression model on all predictors, but shrink or regularize the coefficient estimates to make sure that the model does not \"overfit\" the training set. \n",
    "\n",
    "<div class='exercise'> <b> Question 2 </b> </div> \n",
    "  We're going to use Ridge and Lasso regression regularization techniques to fit linear models to the training set.  We'll use cross-validation and shrinkage parameters $\\lambda$ from the set $\\{.001,.005,1,5,10,50,100,500,1000\\}$ to pick the best model for each regularization technique.\n",
    "\n",
    "**2.1** Use 5-fold cross-validation to pick the best shrinkage parameter from the set $\\{.001,.005,1,5,10,50,100,500,1000\\}$ for your Ridge Regression model on the training data.  Fit a Ridge Regression model on the training set with the selected shrinkage parameter and store your fitted model in the variable `BikeRRModel`.  Store the selected shrinkage parameter in the variable `BikeRR_shrinkage_parameter`.\n",
    "\n",
    "**2.2** Use 5-fold cross-validation to pick the best shrinkage parameter from the set $\\{.001,.005,1,5,10,50,100,500,1000\\}$ for your Lasso Regression model on the training data.  Fit a Lasso Regression model on the training set with the selected shrinkage parameter and store your fitted model in the variable `BikeLRModel`.  Store the selected shrinkage parameter in the variable `BikeLR_shrinkage_parameter`.\n",
    "\n",
    "**2.3** Create three dictionaries `BikeOLSparams`, `BikeLRparams`, and `BikeRRparams`.  Store in each the corresponding regression coefficients for each of the regression models indexed by the string feature name.\n",
    "\n",
    "**2.4** For the Lasso and Ridge Regression models list the features that are assigned a coefficient value close to 0 (i.e. the absolute value of the coefficient is less than 0.1).  How closely do they match the redundant predictors found (if any) in HW 3, Question 5?\n",
    "\n",
    "**2.5** To get a visual sense of how the features different regression models (Multiple Linear Regression, Ridge Regression, Lasso Regression) estimate coefficients, order the features by magnitude of the estimated coefficients in the Multiple Linear Regression Model (no shrinkage).  Plot a bar graph of the magnitude (absolute value) of the estimated coefficients from Multiple Linear Regression in order from greatest to least.  Using a different color (and alpha values) overlay bar graphs of the magnitude of the estimated coefficients (in the same order as the Multiple Linear Regression coefficients) from Ridge and Lasso Regression.\n",
    "\n",
    "**2.6** Let's examine a pair of features we believe to be related.  Is there a difference in the way Ridge and Lasso regression assign coefficients to the predictors `temp` and `atemp`? If so, explain the reason for the difference.\n",
    "\n",
    "**2.7** Discuss the Results:\n",
    "\n",
    "1. How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without shrinkage penalty) in Question 1? \n",
    "2. Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference.\n",
    "3. Is the significance related to the shrinkage in some way?\n",
    "\n",
    "*Hint:* You may use `sklearn`'s `RidgeCV` and `LassoCV` classes to implement Ridge and Lasso regression. These classes automatically perform cross-validation to tune the parameter $\\lambda$ from a given range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [.001, .005, 1, 5, 10, 50, 100, 500, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Use 5-fold cross-validation to pick the best shrinkage parameter from the set $\\{.001,.005,1,5,10,50,100,500,1000\\}$ for your Ridge Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Use 5-fold cross-validation to pick the best shrinkage parameter from the set $\\{.001,.005,1,5,10,50,100,500,1000\\}$ for your Lasso Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Create three dictionaries `BikeOLSparams`, `BikeLRparams`, and `BikeRRparams`.  Store in each the corresponding regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 For the Lasso and Ridge Regression models list the features that are assigned a coefficient value close to 0 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 To get a visual sense of how the features different regression models (Multiple Linear Regression, Ridge Regression, Lasso Regression) estimate coefficients, order the features by magnitude of the estimated coefficients in the Multiple Linear Regression Model (no shrinkage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Let's examine a pair of features we believe to be related.  Is there a difference in the way Ridge and Lasso regression assign coefficients ...v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.1 How do the estimated coefficients compare to or differ from ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.2 Is there a difference between coefficients estimated by the two shrinkage methods ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.3 Is the significance related to the shrinkage in some way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 3: Polynomial Features, Interaction Terms, and Cross Validation </b> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to fit a model to include all main effects and polynomial terms for numerical predictors up to the $4^{th}$ order. More precisely use the following terms: \n",
    " - predictors in `X_train` and `X_test`\n",
    " - $X^1_j$, $X^2_j$, $X^3_j$, and $X^4_j$ for each numerical predictor $X_j$\n",
    "\n",
    "**3.1** Create an expanded training set including all the desired terms mentioned above. Store that training set (as a pandas dataframe) in the variable `X_train_poly`.  Create the corresponding test set and store it as a pandas dataframe in `X_test_poly`.\n",
    "\n",
    "**3.2** Discuss the following:\n",
    "\n",
    "1. What are the dimensions of this 'design matrix' of all the predictor variables in 3.1?  \n",
    "2. What issues may we run into attempting to fit a regression model using all of these predictors?\n",
    "\n",
    "**3.3** Let's try fitting a regression model on all the predictors anyway.  Use the `LinearRegression` library from `sklearn` to fit a multiple linear regression model to the training set data in `X_train_poly`.  Store the fitted model in the variable `BikeOLSPolyModel`.\n",
    "\n",
    "**3.4** Discuss the following:\n",
    "1. What are the training and test $R^2$ scores? \n",
    "2. How does the model performance compare with the OLS model on the original set of features in Question 1?\n",
    "\n",
    "**3.5** The training set $R^2$ score we generated for our model with polynomial and interaction terms doesn't have any error bars.  Let's use cross-validation to generate sample sets of $R^2$ for our model. Use 5-fold cross-validation to generate $R^2$ scores for the multiple linear regression model with polynomial terms.  What are the mean and standard deviation of the $R^2$ scores for your model.\n",
    "\n",
    "**3.6** Visualize the $R^2$ scores generated from the 5-fold cross validation as a box and whisker plot.\n",
    "\n",
    "**3.7** We've used cross-validation to generate error bars around our $R^2$ scores, but another use of cross-validation is as a way of model selection.  Let's construct the following model alternatives:\n",
    "\n",
    "1. Multiple linear regression model generated based upon the feature set in Question 1 (let's call these the base features.\n",
    "2. base features plus polynomial features to order 2\n",
    "3. base features plus polynomial features to order 4\n",
    "\n",
    "Use 5-fold cross validation on the training set to select the best model.  Make sure to evaluate all the models as much as possible on the same folds.  For each model generate a mean and standard deviation for the $R^2$ score.\n",
    "\n",
    "**3.8** Visualize the $R^2$ scores generated for each model from 5-fold cross validation in box and whiskers plots.  Do the box and whisker plots influence your view of which model was best?\n",
    "\n",
    "**3.9** Evaluate each of the model alternatives on the test set.  How do the results compare with the results from cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Create an expanded training set including all the desired terms mentioned above. Store that training set (as a numpy array) in the variable `X_train_poly`...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_train, X_poly_test = get_poly_dataset(X_train, X_test, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 What are the dimensions of this 'design matrix'...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 What issues may we run into attempting to fit a regression model using all of these predictors? ...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Let's try fitting a regression model on all the predictors anyway.  Use the `LinearRegression` library from `sklearn` to fit a multiple linear regression model ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 What are the training and test $R^2$ scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 How does the model performance compare with the OLS model on the original set of features in Question 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 The training set $R^2$ score we generated for our model with polynomial and interaction terms doesn't have any error bars.  Let's use cross-validation to generate sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Visualize the $R^2$ scores generated from the 5-fold cross validation as a box and whisker plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 We've used cross-validation to generate error bars around our $R^2$ scores, but another use of cross-validation is as a way of model selection.  Let's construct the following model alternatives ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 Visualize the $R^2$ scores generated for each model from 5-fold cross validation in box and whiskers plots.  Do the box and whisker plots influence your view of which model was best? ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 Evaluate each of the model alternatives on the  test set.  How do the results compare with the results from cross-validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "Your answer here\n",
    "<HR>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
